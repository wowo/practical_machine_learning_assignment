---
title: "Classification of weight lifting correctness exercise based on Human Activity Recognition"
author: "Wojciech Sznapka"
date: "21.11.2014"
output:
  html_document:
    fig_caption: yes
    highlight: tango
---

## Introduction

The prompt is to predict quality  of human's activity based on measurements collected from several sensors located on different parts of body. Six participants were performing Unilateral Dumbbell Biceps exercises, while wearing sensors and being assessed by experienced weight lifter. The sensors were located on arm, forearm, belt and dumbbell (see Figure 1). Participants were doing exercises in five different manners, described as classes A, B, C, D, E. The classes mean following:

* A - correct execution of exercise
* B - throwing the elbows to the front 
* C - lifting the dumbbell only halfway
* D - lowering the dumbbell only halfway
* E - throwing the hips to the front.

![Figure 1. On-body sensing schema.](on-body-sensing-schema.png)

## Loading Data

Two data-sets (training and test) were downloaded from <http://coursera.org> website, from **Practical Machine Learning** class resources on 21.11.2014. The training data for this project are available here: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>. The test data are available here: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>. The original source of the data is located here: <http://groupware.les.inf.puc-rio.br/har>. The training set contains **classe** variable, which determines if given measurement were performed correctly (class A) or were faulty (B/C/D/E). The test set doesn't provides classe variable. It needs to be predicted.

Whole data processing were done using R software (R version 3.0.2 (2013-09-25)) under 32-bit Ubuntu Linux (4x2,4 GHz CPU, 8GB of RAM). Following R packages were used to perform analysis: Caret, randomForest and mlearning.

```{r, cache=TRUE}
library(caret)
library(randomForest)
library(mlearning)
```

The data was loaded from CSV and preliminary examined to figure out it's size and specification.

```{r, cache=TRUE}
originalTrain <- read.csv(file = 'pml-training.csv');
finalTest <- read.csv(file = 'pml-testing.csv');
```

```{r, cache=TRUE}
dim(originalTrain)
```

The provided train data-set was split into training set (0.75) and validation set (0.25) based on classe variable, in order to build model and validate its accuracy.

```{r, cache=TRUE}
partition <- createDataPartition(originalTrain$classe, p=.75, list=F)
train <- originalTrain[partition,]
validation <- originalTrain[-partition,]
```

Exploratory analysis of the data showed that some columns doesn't play significant role in model building. Thus we decided to drop them for further processing.

```{r, cache=TRUE}
drops <- c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window')
train <- train[, !(names(train) %in% drops)]
validation <- validation[, !(names(validation) %in% drops)]
finalTest <- finalTest[, !(names(finalTest) %in% drops)]
```

What's more, we decided to remove another set of columns, which were returned by near zero var method, because they wouldn't be significant for the predictive model.

```{r, cache=TRUE}
colsToDrop <- nearZeroVar(train)
dim(colsToDrop)

train <- train[, -colsToDrop]
validation <- validation[, -colsToDrop]
finalTest <- finalTest[, -colsToDrop]
```

The last step of data preparation was dealing with NA values. Some columns were very sparse, meaning they contained a lot of NA's. That caused inability to predict values for rows containing NA. We fixed that situation by putting zeros instead of NA's

```{r, cache=TRUE}
train[is.na(train)] <- 0
validation[is.na(validation)] <- 0
finalTest[is.na(finalTest)] <- 0
```

## Building Model

To make the research reproducible, we set seed to fixed value.

```{r, cache=TRUE}
set.seed(12345)
```

We decided to use Random Forest [2] classification algorithm to build model which will be used for predicting.
> Random forests are an ensemble learning method for classification (and regression) that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes output by individual trees. 

> Decision trees are a popular method for various machine learning tasks. Tree learning "come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining", say Hastie et al., because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelavant features, and produces inspectable models. However, they are seldom accurate.[3]

> In particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, because they have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance.[3] This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model.

The model was build on training data and validated on validation set. Then it was used to solve actual problem - predict class for 20 activities in test set. The usage of caret 's train method n(with parameter `method="rf"`) was impossible due to problems with memory allocation. Calling `randomForest` method directly made significant improvement in performance. Model was build based on **classe** variable and all remaining features in train set.

In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. [5]


```{r, cache=TRUE}
modFit <- randomForest(classe ~ ., data=train)
```

We can check which variables played significant role in prediction bu using `varImpPlot` method. It plots dot-chart of variable importance as measured by a Random Forest. It illustrates importance in **mean decrease in Gini**, which means:

> Every time a split of a node is made on variable m the gini impurity criterion for the two descendent nodes is less than the parent node. Adding up the gini decreases for each individual variable over all trees in the forest gives a fast variable importance that is often very consistent with the permutation importance measure.. [4]

```{r, cache=TRUE, fig.cap='Figure 2. Variables importance'}
varImpPlot(modFit)
```

We can see that belt, dumbbell and forearm give important measurements, while arm isn't that important here.

## Analysis

Using previously created model, we predicted values on training and validation sets. The results proved that usage of Random Forest were right choice. We used `confusion` and `confusionImage` to visualize Confusion Matrix, to check how model performs and how many mistakes it produces.

```{r, cache=TRUE, fig.cap='Figure 3. Confussion matrix for training data set.'}
predTrain <- predict(modFit, train)
confusionImage(confusion(predTrain, train$classe))
```

```{r, cache=TRUE, fig.cap='Figure 4. Confussion matrix for validation data set.'}
predValidation <- predict(modFit, validation)
confusionImage(confusion(predValidation , validation$classe))
```

The accuracy of the model can be calculated as:
```{r, cache=TRUE}
accuracy <- confusionMatrix(table(predValidation, validation$classe))$overall['Accuracy']
accuracy
```

## Conclusion

Accuracy of **`r round(accuracy, 4) * 100`%** indicates that model based on Random Forests preform very well and is very accurate. The model has been used for test set provided for automatic grading to predict class of 20 measurements.

```{r, cache=TRUE}
finalTest$prediction <- predict(modFit, finalTest)
```

Predicted values for given test set has been uploaded to Coursera platform for automatic grading. All 20 samples were predicted correctly, which proved that we build right model.

## References
[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

[2] Random Forest on Wikipedia; <http://en.wikipedia.org/wiki/Random_forest>

[3] Hastie, Trevor; Tibshirani, Robert; Friedman, Jerome (2008). The Elements of Statistical Learning (2nd ed.). Springer. ISBN 0-387-95284-5.

[4] Gini importance, Random Forests, Leo Breiman and Adele Cutler; <https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#giniimp>

[5] The out-of-bag (oob) error estimate, Random Forests, Leo Breiman and Adele Cutler; <https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#workings>